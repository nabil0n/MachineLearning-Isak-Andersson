{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Först en rätt lång genomgång på tidigiare grejer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- allmän linjär regression\n",
    "    - Finn ett hyperplan i p dimensioner: Y = $\\beta_0 + $ osv där vi optimerar en kostnadsfunktion.\n",
    "\n",
    "- olika x distirbutioner:\n",
    "- De viktigaste för oss:\n",
    "    - X = normalfördelad (gaussian) 'vanlig' linjär regression\n",
    "    - X = binomial, logistisk regression\n",
    "- Några till (de funkar ju väldigt likt):\n",
    "    - X = poisson, räknar förekomster (han nämnde ingen regression här, men finns)\n",
    "    - X = gamma, har lång tail. Bra på att hitta undantagsfall. Saker som händer sällan men regelbundet.\n",
    "    - X = neg. binomial. Bra för tidsperioder, ackumulation\n",
    "    - X = invers gauss. Bra på slumpmässiga rörelser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kostnadsfunktioner ger oss regressionsmetoder:\n",
    "    - OLS, Ridge (l2), Lasso(l1) mfl\n",
    "\n",
    "- Att göra en regression är i värsta fallet $O(n^3)$\n",
    "- Men vanligaste är SVD (single value decomposition), inte gått genom. $O(np^2)$ där n är stickprovstorlek och p är antalet features.\n",
    "    - Billigare om vi har mer rader än features, men lika dyrt om n = p såklart då det bli $n^3$ igen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notera att våra parametrar, säg ($x_1, x_2$) mycket väl kan vara icke-linjära i y.\n",
    "    - $y=\\beta_x$ är linjär, men x kan ha mycket mer features än paramterar. tex polynom expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Liten featuremängd generaliserar bättre.\n",
    "    - ockhams razor, det vill säga reagerar bättre på okänd data. Underviker overfit.\n",
    "\n",
    "- Hur gör vi då för att minska antalet dimensioner (features)?\n",
    "    - Forward selection (dyr)\n",
    "        - Börjar med enkel linjär regression på varje parameter ($\\beta$)\n",
    "        - Välj den som ger bäst $R^2 / MSE$\n",
    "        - Lägg till fler tills $R^2 / MSE$ inte blir bättre\n",
    "    - Backwards elimination (dyr)\n",
    "        - Börja med alla p parametrar.\n",
    "        - För varje p-1 parameter testa $R^2 /MSE$\n",
    "        - Ta bort parametrar till $R^2 / MSE$ blir bättre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA - Principal Component Analasys\n",
    "ISLP kapitel 12.2\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
    "\n",
    "- Data-analys (dimensionsreducering)\n",
    "- automatic feature engineering\n",
    "- oövervakad inlärning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Han ritar en graf med röda prickar med två blåa linjer igenom, linjerna är principalkomponenterna.\n",
    "- Hitta en bas till X så att vi kan göra en vanlig linjär regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi behöver fortfarande välja ut några Z (x) av alla för att vinna något på det. (förstod inte detta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vi har ingen responsvektor, inga labelar, inga rätta svar! Alltså inga Y! == Oövervakad inlärning!\n",
    "- Oövervakad inlärning är bra för grupperingar och genereringar.\n",
    "    - Det brukar vara första steget för autonoma system. Först gör detta utan labelar osv för att sedan ladela upp och göra ett övervakad system uppepå."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det finns en regressions variant som heter PCR, står mer i boken om det. Den har inga stora fördelar över svd osv, historiskt hade den det men inte längre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notera: Varje principalkomponent efter den första förklarar mindre och mindre av variansen i datan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning-Isak-Andersson-nqPwo2ln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
