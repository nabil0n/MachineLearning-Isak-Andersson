{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro till GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Det fanns något som hetter NLP\n",
    "    - klassiska tekniker (tekniker från innan 2017) räcker inte längre\n",
    "\n",
    "- Nu körs istället GPT\n",
    "    - Står för Generative Pre-trained Transformed, där Pre-trained är det viktiga här.\n",
    "- En transform är en encoder -> dolda lager -> decoder\n",
    "    - encoder (BERT)\n",
    "    - decoder (GPT) (decoder-only)\n",
    "\n",
    "- Embedding:\n",
    "    - Omvandlar ord till siffror.\n",
    "    - Vi kallar dem då tokens.\n",
    "    - För språktydning:\n",
    "        - ALLA ord och vanliga fraser blir sina egna noder (tokens)\n",
    "        - 100tusentals noder med miljontals kopplingar\n",
    "        - Attention:\n",
    "            - I en given mening: \"Varför går du inte in i det röda huset?\"\n",
    "            - Vi tittar på ett ord i taget, men när vi tittar på det tittar vi på likheten till de andra orden också.\n",
    "            - Det görs en skalärprodukt mellan embedded ord tokens. Vi får ett värde för likheten mellan alla ord.\n",
    "            - Inte en perfekt metod.\n",
    "                - Negatitionsord och syftningar är svårt.\n",
    "        - Self-attention (masked self-attention)\n",
    "            - Vi tittar endast åt ett håll\n",
    "            - Som vi läser, vi jämför (i svenska och engelska) med orden åt vänster.\n",
    "            - Så i meningen ovan jämförs först \"varför\" med \"varför\", sedan \"varför\" med \"går\" osv.\n",
    "                - Detta returnerar någon numeriskt värde per jämförelse.\n",
    "                - Detta blir en matris med värden som är nätverkets förståelse av vad som är viktigt (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allt vi nämnt hittils är automatiskt och oövervakat.\n",
    "- Vad är då Pre-Trained?\n",
    "    - Optimeringsvilkor för oövervakad inlärning\n",
    "\n",
    "### $max \\boldsymbol{\\Sigma} log P(u_i | u_{i-k}, ..., u_{i-1}; \\theta)$\n",
    "där $\\theta$ är ett neuralt nätverk\n",
    "\n",
    "- Vi lär alltså systemet att läsa först, sedan tränar vi övervakat. Meningar | Svar\n",
    "    - Riktiga människor har suttit och labelat mycket data som möjligt.\n",
    "    - Alltså gett svar för meningar att relatera till.\n",
    "- Då lär nätverket sig domänkunskap genom att kunna förutsätta svaret. Assosiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT använder SAMMA decoder överallt i nätverket.\n",
    "    - Detta är det viktiga.\n",
    "- Kan ge ett liknande svar till en liknande mening.\n",
    "- Ingen vet hur det funkar (så bra?) under ytan."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
