{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "*Stochastic*:\n",
    "https://scikit-learn.org/stable/modules/sgd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tanken är väldigt enkel: Ta ett \"litet\" steg i en \"bra\" riktning, om och om igen, för att nå minimum.\n",
    "- Den är alltså iterativ, den tar flera steg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Differensekvation: iterar över tid.\n",
    "- Differentialekvation: itegragerar över kontinuerlig tid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistisk (iterativ): $x_n+1 = r(1-x_n)$\n",
    "    - Den hoppar mellan r värden (om r=2 så blir det som en \"såg\" typ)\n",
    "    - r = 3.58 är bra (?)\n",
    "    - Den där numberphile videon där det delas på två hela tiden, men bryter alltså ihop på 3.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jacobian matrix: En matris med derivator.\n",
    "- En vektor av derivator pekar alltid i den riktning som har störts förändringshastighet. Den riktning vi vill \"gå\" i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OLS:\n",
    "    - $C(\\beta) = MSE$\n",
    "- Ridge:\n",
    "    - $C(\\beta) = MSE + \\lambda\\Sigma\\beta_i^2$\n",
    "- Gradient Descent:\n",
    "    - $C(\\theta) = \\frac{1}{n-k-1}\\Sigma(y_i - \\theta * x_i)^2$\n",
    "    - Batch GD:\n",
    "        - $\\theta_{i+2} = \\theta_i - \\eta \\nabla C(\\theta)$\n",
    "        - $\\theta_{i+1} = \\theta_i - \\eta(\\frac{2}{n-k-1}\\boldsymbol{x}^t (\\boldsymbol{x}\\theta_i -y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stochastic GD:\n",
    "    - Den väljer slumpmässigt vart vi ska gå.\n",
    "    - Välj slumpmässigt en punkt och beräkna $\\nabla$ på den punkten.\n",
    "    - Upprepa givet antal ggr\n",
    "    - $\\eta$ är hur stora steg vi tar, detta kan göra att man hoppar över en godnedåtriktning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minibatch GD:\n",
    "    - Välj delmängd av punkter och räkna gradient.\n",
    "    - Annars som tidigare (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent with Momentum:\n",
    "    - Den mest använda.\n",
    "    - Fysikinspirerad.\n",
    "    - $x_{k+1} = x_k - sz_k$\n",
    "    - $z_k = \\nabla f_k + \\beta z_{k-1}$\n",
    "    - ADAM i sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning-Isak-Andersson-0nAm_eHr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
