{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro till (artificiella) neurala nätverk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det vi gjort hittils:\n",
    "- Klassisk maskininlärning\n",
    "    - regression; statistiska metoder\n",
    "\n",
    "Neurala nätverk:\n",
    "- Baserat på en föråldrad syn på hjärnan. Men funkar hyffsat.\n",
    "\n",
    "- Perceptron:\n",
    "    - noder kopplade till varandra.\n",
    "\n",
    "- Linjens ekvation: $y = w_1x_1 + w_2x_2 + b$'\n",
    "    - \"2d-linje\" - plan som spänns upp av **w**\n",
    "- Men i ett mer avancerat nätverk:\n",
    "    - $y = 1(1x_1 + 3x_2)+2(2x_3 + 1x_4)$\n",
    "    - som omsrkivet är \n",
    "    - $y = 1x_1 + 3x_2 + 4x_3 + 1x_4$\n",
    "    - Dessa är linjära och därför hjälper oss inte, vi kan inte sätta ihop dem till att bli bättre\n",
    "    - Vi måste hitta sätt att göra dem icke-linjära\n",
    "\n",
    "- Vi härmar mer modern hjärnforskning (?)\n",
    "    - Vikter vid synapskopplingarna (?)\n",
    "    - Lägga till sigmoid (mjuk linje på grafen) eller ReLU (hårdare knä) (finns även GeLU men den pratade vi inte om)\n",
    "    - Vi lägger till fler lager (fler $x_n$)\n",
    "    - Ny standard kom (50-80-tal): MLP (Multi-Layered Perceptrons)\n",
    "        - Alla noder kopplade till alla noder (typ, itne alltid)\n",
    "        - Hidden layers\n",
    "        - När detta kom fanns inte teorin hur man tränar nätverken.\n",
    "        - Det var dessutom beräkningsmässigt vääälidgt dyrt. + att mycket räknades för hand eller på datorns barnsben.\n",
    "    - Formeln för nätverket:\n",
    "    - $f(x) = V\\sigma(Wx + b) + c$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- De dolda lagren gör en feature expansion (de skapar och utvecklar features), och de ytliga lagren (nära y) gör en regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Det behövs en algoritm för att träna sin nätverk\n",
    "    - Hur gör vi?\n",
    "        - Vi sätter slumpmässiga vikter (alla w) (linjerna mellan noderna)\n",
    "        - Sedan testar vi y - y_pred och justerar vikterna litegrann för att y - y_pred ska bli mindre och mindre.\n",
    "        - SGD!\n",
    "        - Gradient descent funkar! Men är gaaaalet dyrt, omöjligt dyrt.\n",
    "    - Hela kruxet med djupinlärning idag är att komma runt den kostnaden.\n",
    "        - Chip som inte bara blir snabbare utan även kan göramånga saker samtidigt, parallellprogrammering eller nåt sa han\n",
    "    - Backpropagation:\n",
    "        - numerisk derivering\n",
    "            - Går fort!\n",
    "        - Gör SGD en gång, läser y, går bakåt i lagren, gör SGD med nya vikter, repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM kan i vissa sammanhang tävla med neurala nätverk i att lösa problem.\n",
    "    - expanderar featurerymden väldigt mycket, kollapsar dimensioner för att undvika overfit.\n",
    "    - expanderar til hög dimension och hoppas på att vi hittar en linjär lösning.\n",
    "- i NN lär vi istället nätverket features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(parentes om vektordatorer)\n",
    "- parallellism\n",
    "    - vektorer med instruktioner.\n",
    "    - vilket är så neurala nätverk egentlgien fungerar.\n",
    "    - tidiga cpu var typ g4 från IBM som apple använde tidigt.\n",
    "    - kompilatorer för vektorinstruktioner är vääälidgt svårt att skriva"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
